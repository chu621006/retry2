import streamlit as st
import pandas as pd
import pdfplumber
import collections
import re

# --- è¼”åŠ©å‡½æ•¸ ---
def normalize_text(cell_content):
    """
    æ¨™æº–åŒ–å¾ pdfplumber æå–çš„å–®å…ƒæ ¼å…§å®¹ã€‚
    è™•ç† None å€¼ã€pdfplumber çš„ Text ç‰©ä»¶å’Œæ™®é€šå­—ä¸²ã€‚
    å°‡å¤šå€‹ç©ºç™½å­—å…ƒï¼ˆåŒ…æ‹¬æ›è¡Œï¼‰æ›¿æ›ç‚ºå–®å€‹ç©ºæ ¼ï¼Œä¸¦å»é™¤å…©ç«¯ç©ºç™½ã€‚
    """
    if cell_content is None:
        return ""

    text = ""
    if hasattr(cell_content, 'text'):
        text = str(cell_content.text)
    elif isinstance(cell_content, str):
        text = cell_content
    else:
        text = str(cell_content)
    
    # å°‡æ‰€æœ‰ç©ºç™½å­—å…ƒæ›¿æ›ç‚ºå–®å€‹ç©ºæ ¼ï¼Œä¸¦å»é™¤å‰å¾Œç©ºç™½
    return re.sub(r'\s+', ' ', text).strip()

def make_unique_columns(columns_list):
    """
    å°‡åˆ—è¡¨ä¸­çš„æ¬„ä½åç¨±è½‰æ›ç‚ºå”¯ä¸€çš„åç¨±ï¼Œè™•ç†é‡è¤‡å’Œç©ºå­—ä¸²ã€‚
    å¦‚æœé‡åˆ°é‡è¤‡æˆ–ç©ºå­—ä¸²ï¼Œæœƒæ·»åŠ å¾Œç¶´ (ä¾‹å¦‚ 'Column_1', 'æ¬„ä½_2')ã€‚
    """
    seen = collections.defaultdict(int)
    unique_columns = []
    for col in columns_list:
        # æ¸…ç†æ¬„ä½åç¨±ï¼Œç§»é™¤å¤šé¤˜ç©ºæ ¼å’Œç‰¹æ®Šç¬¦è™Ÿ
        original_col_cleaned = normalize_text(col)
        
        # å¦‚æœæ¸…ç†å¾Œé‚„æ˜¯ç©ºçš„ï¼Œçµ¦å€‹é€šç”¨åç¨±
        if not original_col_cleaned:
            # å„ªå…ˆä½¿ç”¨åµæ¸¬åˆ°çš„Column_Xï¼Œç„¶å¾Œå†éå¢
            column_name_base = "Column_"
            current_idx = 1
            while f"{column_name_base}{current_idx}" in unique_columns:
                current_idx += 1
            name = f"{column_name_base}{current_idx}"
        else:
            name = original_col_cleaned
        
        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼Œä¸¦ç”Ÿæˆå”¯ä¸€åç¨±
        counter = seen[name]
        final_name = name
        while final_name in unique_columns: # é¿å…ç”Ÿæˆ Column_1_1, è€Œç›´æ¥ Column_2
            counter += 1
            final_name = f"{name}_{counter}" if counter > 1 else name
        
        # ç¢ºä¿æœ€çµ‚åç¨±çœŸçš„æ˜¯å”¯ä¸€çš„ï¼Œå› ç‚ºå­˜åœ¨ Column_1, Column_1_1, Column_1_2 çš„æƒ…æ³
        # å¦‚æœ name æœ¬èº«æ˜¯ Column_X å½¢å¼ï¼Œå‰‡ç›´æ¥ä½¿ç”¨æ–°çš„å”¯ä¸€æ•¸å­—
        if re.match(r"Column_\d+", name) and counter > 0:
            current_col_num = int(name.split('_')[-1]) if name.split('_')[-1].isdigit() else 0
            final_name = f"Column_{max(current_col_num, len(unique_columns)) + 1}"
            while final_name in unique_columns:
                final_name = f"Column_{int(final_name.split('_')[-1]) + 1}"
        elif counter > 0:
            final_name = f"{name}_{counter}"
        
        unique_columns.append(final_name)
        seen[name] = counter 

    return unique_columns


def calculate_total_credits(df_list):
    """
    å¾æå–çš„ DataFrames åˆ—è¡¨ä¸­è¨ˆç®—ç¸½å­¸åˆ†ã€‚
    å°‹æ‰¾åŒ…å« 'å­¸åˆ†' æˆ– 'å­¸åˆ†(GPA)' é¡ä¼¼å­—æ¨£çš„æ¬„ä½é€²è¡ŒåŠ ç¸½ã€‚
    """
    total_credits = 0.0
    
    st.subheader("å­¸åˆ†è¨ˆç®—åˆ†æ")

    # å®šç¾©å¯èƒ½çš„å­¸åˆ†æ¬„ä½åç¨±é—œéµå­—
    credit_column_keywords = ["å­¸åˆ†", "å­¸åˆ†æ•¸", "å­¸åˆ†(GPA)", "å­¸ åˆ†", "Credits"] 
    
    # ç”¨æ–¼å¾å¯èƒ½åŒ…å«GPAçš„å­—ç¬¦ä¸²ä¸­æå–æ•¸å­—å­¸åˆ†ï¼Œä¾‹å¦‚ "A 2" -> 2, "3" -> 3
    # å°‹æ‰¾å­—ä¸²ä¸­æ‰€æœ‰å¯èƒ½çš„æ•¸å­— (æ•´æ•¸æˆ–æµ®é»æ•¸)ï¼Œä¸¦å–æœ€å¾Œä¸€å€‹ï¼ˆé€šå¸¸æ˜¯å­¸åˆ†ï¼‰
    credit_pattern = re.compile(r'(\d+(\.\d+)?)') 

    for df_idx, df in enumerate(df_list):
        st.write(f"--- åˆ†æè¡¨æ ¼ {df_idx + 1} ---")
        st.write(f"åµæ¸¬åˆ°çš„åŸå§‹æ¬„ä½åç¨±: `{list(df.columns)}`") 
        
        found_credit_column = None
        
        # å„ªå…ˆåŒ¹é…æ˜ç¢ºçš„å­¸åˆ†é—œéµå­—
        for col in df.columns:
            cleaned_col_for_match = "".join(char for char in col if '\u4e00' <= char <= '\u9fa5' or 'a' <= char <= 'z' or 'A' <= char <= 'Z' or '0' <= char <= '9').strip()
            if any(keyword in cleaned_col_for_match for keyword in credit_column_keywords):
                found_credit_column = col 
                break
        
        # å¦‚æœæ²’æœ‰æ˜ç¢ºåŒ¹é…ï¼Œå˜—è©¦å¾é€šç”¨åç¨± (Column_X) ä¸­çŒœæ¸¬å­¸åˆ†æ¬„ä½
        if not found_credit_column:
            # å°‹æ‰¾å¯èƒ½åŒ…å«æ•¸å­—çš„ Column_X æ¬„ä½
            potential_credit_columns = []
            for col in df.columns:
                if re.match(r"Column_\d+", col):
                    # æª¢æŸ¥è©²æ¬„ä½çš„å‰å¹¾è¡Œæ•¸æ“šæ˜¯å¦å¤§éƒ¨åˆ†æ˜¯æ•¸å­—æˆ–å¯è½‰æ›ç‚ºæ•¸å­—
                    # å–å‰5è¡Œæ•¸æ“šé€²è¡Œåˆ¤æ–·ï¼Œé¿å…ç©ºè¡Œæˆ–è¡¨å°¾ç¸½è¨ˆçš„å¹²æ“¾
                    sample_data = df[col].head(5).apply(normalize_text).tolist()
                    numeric_count = 0
                    for item_str in sample_data:
                        if item_str == "é€šé" or item_str == "æŠµå…":
                            numeric_count += 1
                        else:
                            matches = credit_pattern.findall(item_str)
                            if matches:
                                try:
                                    float(matches[-1][0])
                                    numeric_count += 1
                                except ValueError:
                                    pass
                    
                    # å¦‚æœè¶…éä¸€åŠçš„æ¨£æœ¬æ•¸æ“šæ˜¯æ•¸å­—ï¼Œå‰‡èªç‚ºå¯èƒ½æ˜¯å­¸åˆ†æ¬„ä½
                    if len(sample_data) > 0 and numeric_count / len(sample_data) > 0.5:
                        potential_credit_columns.append(col)
            
            # å¦‚æœæ‰¾åˆ°å¤šå€‹æ½›åœ¨å­¸åˆ†æ¬„ä½ï¼Œå˜—è©¦æ‰¾å‡ºæœ€åƒå­¸åˆ†çš„
            # é€šå¸¸å­¸åˆ†æœƒåœ¨ç§‘ç›®åç¨±å¾Œé¢å¹¾æ¬„
            if potential_credit_columns:
                # æ‰¾åˆ°ç§‘ç›®åç¨±æ¬„ä½ï¼Œå­¸åˆ†æ¬„ä½æ‡‰è©²åœ¨å…¶å³å´
                subject_name_col_idx = -1
                for i, col in enumerate(df.columns):
                    if "ç§‘ç›®åç¨±" in normalize_text(col):
                        subject_name_col_idx = i
                        break
                
                if subject_name_col_idx != -1:
                    # é¸å–ç§‘ç›®åç¨±å³å´ä¸”æœ€æ¥è¿‘çš„æ½›åœ¨å­¸åˆ†æ¬„ä½
                    for p_col in potential_credit_columns:
                        if df.columns.get_loc(p_col) > subject_name_col_idx:
                            found_credit_column = p_col
                            break
                
                # å¦‚æœæ²’æœ‰ç§‘ç›®åç¨±æ¬„ä½æˆ–å³å´æ²’æœ‰ï¼Œå°±é¸ç¬¬ä¸€å€‹æ½›åœ¨å­¸åˆ†æ¬„ä½
                if not found_credit_column and potential_credit_columns:
                    found_credit_column = potential_credit_columns[0]


        if found_credit_column:
            st.info(f"å¾è¡¨æ ¼ {df_idx + 1} (åŸå§‹æ¬„ä½: '{found_credit_column}') åµæ¸¬åˆ°å­¸åˆ†æ•¸æ“šã€‚")
            try:
                processed_credits = []
                for item in df[found_credit_column]:
                    item_str = normalize_text(item) # ä½¿ç”¨æ¨™æº–åŒ–å‡½æ•¸è™•ç†æ•¸æ“šå–®å…ƒæ ¼
                    
                    credit_val = 0.0
                    # å„ªå…ˆè™•ç†å·²çŸ¥éæ•¸å­—çš„å­¸åˆ†æƒ…æ³
                    if item_str == "é€šé" or item_str == "æŠµå…":
                        credit_val = 0.0
                    else:
                        # å˜—è©¦ç”¨æ­£å‰‡è¡¨é”å¼å¾å­—ä¸²ä¸­æå–æ‰€æœ‰æ•¸å­—
                        matches = credit_pattern.findall(item_str)
                        if matches:
                            # å‡è¨­æœ€å¾Œä¸€å€‹æ•¸å­—é€šå¸¸æ˜¯å­¸åˆ†ï¼Œä¾‹å¦‚ "A 2" ä¸­çš„ "2"
                            try:
                                credit_val = float(matches[-1][0]) 
                            except ValueError:
                                credit_val = 0.0
                        else:
                            credit_val = 0.0 # æ²’æœ‰åŒ¹é…åˆ°æ•¸å­—
                    
                    processed_credits.append(credit_val)

                credits_series = pd.Series(processed_credits)
                
                valid_credits = credits_series[credits_series >= 0] 
                
                current_table_credits = valid_credits.sum()
                total_credits += current_table_credits
                st.write(f"è¡¨æ ¼ {df_idx + 1} çš„å­¸åˆ†ç¸½è¨ˆ: **{current_table_credits:.2f}**")
                
            except Exception as e:
                st.warning(f"è¡¨æ ¼ {df_idx + 1} çš„å­¸åˆ†æ¬„ä½ '{found_credit_column}' è½‰æ›ç‚ºæ•¸å€¼æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e}`")
                st.warning("è©²è¡¨æ ¼çš„å­¸åˆ†å¯èƒ½ç„¡æ³•è¨ˆå…¥ç¸½æ•¸ã€‚è«‹æª¢æŸ¥å­¸åˆ†æ¬„ä½æ•¸æ“šæ˜¯å¦ç‚ºç´”æ•¸å­—æˆ–å¯æå–æ•¸å­—ã€‚")
        else:
            st.info(f"è¡¨æ ¼ {df_idx + 1} æœªåµæ¸¬åˆ°æ˜ç¢ºçš„å­¸åˆ†æ¬„ä½ã€‚æª¢æŸ¥æ¬„ä½ï¼š`{list(df.columns)}`ã€‚ä¸è¨ˆå…¥ç¸½å­¸åˆ†ã€‚")
            
    return total_credits

def process_pdf_file(uploaded_file):
    """
    ä½¿ç”¨ pdfplumber è™•ç†ä¸Šå‚³çš„ PDF æª”æ¡ˆï¼Œæå–è¡¨æ ¼ã€‚
    """
    all_grades_data = []

    try:
        with pdfplumber.open(uploaded_file) as pdf:
            st.write(f"æ­£åœ¨è™•ç†æª”æ¡ˆ: **{uploaded_file.name}**")
            num_pages = len(pdf.pages)
            st.info(f"PDF ç¸½é æ•¸: **{num_pages}**")

            for page_num, page in enumerate(pdf.pages):
                st.subheader(f"é é¢ {page_num + 1}")

                table_settings = {
                    "vertical_strategy": "lines",
                    "horizontal_strategy": "lines",
                    "snap_tolerance": 3,
                    "join_tolerance": 3,
                    "edge_min_length": 3,
                    "text_tolerance": 1,
                    # å¯ä»¥å˜—è©¦èª¿æ•´é€™äº›åƒæ•¸ä¾†å„ªåŒ–è¡¨æ ¼åµæ¸¬
                    # "intersection_tolerance": 5, 
                    # "min_words_vertical": 1, 
                    # "min_words_horizontal": 1,
                }
                
                current_page = page
                
                # --- é‡å°ç‰¹å®š PDF å’Œé é¢é€²è¡Œ bbox èª¿æ•´ ---
                # **é‡è¦ï¼šæ‚¨éœ€è¦æ ¹æ“šå¯¦éš›PDFå…§å®¹ï¼Œæ‰‹å‹•æ¸¬é‡ç²¾ç¢ºçš„bboxåæ¨™**
                # åæ¨™æ ¼å¼ç‚º (x0, y0, x1, y1)
                # x0, y0 æ˜¯å·¦ä¸Šè§’åæ¨™ï¼Œx1, y1 æ˜¯å³ä¸‹è§’åæ¨™
                # æ‚¨å¯ä»¥ä½¿ç”¨ PDF é–±è®€å™¨çš„ã€Œé‡æ¸¬å·¥å…·ã€æˆ– pdfplumber çš„ debug æ¨¡å¼ä¾†ç²å–é€™äº›åæ¨™
                if "è¬äº‘ç‘„æˆç¸¾ç¸½è¡¨.pdf" in uploaded_file.name:
                    if page_num + 1 == 3: # è¬äº‘ç‘„æˆç¸¾ç¸½è¡¨.pdf çš„ç¬¬ 3 é 
                        # ç¯„ä¾‹åæ¨™ï¼Œè«‹æ›¿æ›ç‚ºæ‚¨æ¸¬é‡åˆ°çš„ç²¾ç¢ºå€¼
                        # å»ºè­°æ‚¨å˜—è©¦è¼ƒå¤§çš„ç¯„åœï¼Œç„¶å¾Œé€æ­¥ç¸®å°
                        st.warning(f"è«‹ç‚ºè¬äº‘ç‘„æˆç¸¾ç¸½è¡¨.pdf é é¢ {page_num + 1} æä¾›ç²¾ç¢ºçš„ bbox åæ¨™ (x0, y0, x1, y1)ã€‚")
                        # current_page = page.crop((x0, y0, x1, y1)) 
                    elif page_num + 1 == 4: # è¬äº‘ç‘„æˆç¸¾ç¸½è¡¨.pdf çš„ç¬¬ 4 é 
                        # ç¯„ä¾‹åæ¨™ï¼Œè«‹æ›¿æ›ç‚ºæ‚¨æ¸¬é‡åˆ°çš„ç²¾ç¢ºå€¼
                        st.warning(f"è«‹ç‚ºè¬äº‘ç‘„æˆç¸¾ç¸½è¡¨.pdf é é¢ {page_num + 1} æä¾›ç²¾ç¢ºçš„ bbox åæ¨™ (x0, y0, x1, y1)ã€‚")
                        # current_page = page.crop((x0, y0, x1, y1))
                # --- çµæŸ bbox èª¿æ•´ ---

                try:
                    tables = current_page.extract_tables(table_settings)

                    if not tables:
                        st.warning(f"é é¢ **{page_num + 1}** æœªåµæ¸¬åˆ°è¡¨æ ¼ã€‚é€™å¯èƒ½æ˜¯ç”±æ–¼ PDF æ ¼å¼è¤‡é›œæˆ–è¡¨æ ¼æå–è¨­å®šä¸é©ç”¨ã€‚")
                        continue

                    for table_idx, table in enumerate(tables):
                        st.markdown(f"**é é¢ {page_num + 1} çš„è¡¨æ ¼ {table_idx + 1}**")
                        
                        processed_table = []
                        for row in table:
                            normalized_row = [normalize_text(cell) for cell in row]
                            processed_table.append(normalized_row)
                        
                        if not processed_table:
                            st.info(f"è¡¨æ ¼ **{table_idx + 1}** æå–å¾Œç‚ºç©ºã€‚")
                            continue

                        # å‡è¨­ç¬¬ä¸€è¡Œæ˜¯æ¨™é¡Œè¡Œï¼Œä½†ç¢ºä¿æœ‰è¶³å¤ çš„è¡Œ
                        if len(processed_table) > 0:
                            header_row = processed_table[0]
                            data_rows = processed_table[1:]
                        else:
                            header_row = []
                            data_rows = []

                        unique_columns = make_unique_columns(header_row)

                        if data_rows:
                            num_columns_header = len(unique_columns)
                            cleaned_data_rows = []
                            for row in data_rows:
                                if len(row) > num_columns_header:
                                    cleaned_data_rows.append(row[:num_columns_header])
                                elif len(row) < num_columns_header:
                                    cleaned_data_rows.append(row + [''] * (num_columns_header - len(row)))
                                else:
                                    cleaned_data_rows.append(row)

                            try:
                                df_table = pd.DataFrame(cleaned_data_rows, columns=unique_columns)
                                all_grades_data.append(df_table)
                                st.dataframe(df_table)
                            except Exception as e_df:
                                st.error(f"é é¢ {page_num + 1} è¡¨æ ¼ {table_idx + 1} è½‰æ›ç‚º DataFrame æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e_df}`")
                                st.error(f"åŸå§‹è™•ç†å¾Œæ•¸æ“šç¯„ä¾‹: {processed_table[:2]} (å‰å…©è¡Œ)")
                                st.error(f"ç”Ÿæˆçš„å”¯ä¸€æ¬„ä½åç¨±: {unique_columns}")
                        else:
                            st.info(f"è¡¨æ ¼ **{table_idx + 1}** æ²’æœ‰æ•¸æ“šè¡Œã€‚")

                except Exception as e_table:
                    st.error(f"é é¢ **{page_num + 1}** è™•ç†è¡¨æ ¼æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e_table}`")
                    st.warning("é€™å¯èƒ½æ˜¯ç”±æ–¼ PDF æ ¼å¼è¤‡é›œæˆ–è¡¨æ ¼æå–è¨­å®šä¸é©ç”¨ã€‚")

    except pdfplumber.PDFSyntaxError as e_pdf_syntax:
        st.error(f"è™•ç† PDF èªæ³•æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e_pdf_syntax}`ã€‚æª”æ¡ˆå¯èƒ½å·²æå£æˆ–æ ¼å¼ä¸æ­£ç¢ºã€‚")
    except Exception as e:
        st.error(f"è™•ç† PDF æª”æ¡ˆæ™‚ç™¼ç”Ÿä¸€èˆ¬éŒ¯èª¤: `{e}`")
        st.error("è«‹ç¢ºèªæ‚¨çš„ PDF æ ¼å¼æ˜¯å¦ç‚ºæ¸…æ™°çš„è¡¨æ ¼ã€‚è‹¥å•é¡ŒæŒçºŒï¼Œå¯èƒ½æ˜¯ PDF çµæ§‹è¼ƒç‚ºè¤‡é›œï¼Œéœ€è¦èª¿æ•´ `pdfplumber` çš„è¡¨æ ¼æå–è¨­å®šã€‚")

    return all_grades_data

# --- Streamlit æ‡‰ç”¨ä¸»é«” ---
def main():
    st.set_page_config(page_title="PDF æˆç¸¾å–®æå–èˆ‡å­¸åˆ†è¨ˆç®—å·¥å…·", layout="wide")
    st.title("ğŸ“„ PDF æˆç¸¾å–®è¡¨æ ¼æ•¸æ“šæå–èˆ‡å­¸åˆ†è¨ˆç®—")

    st.write("è«‹ä¸Šå‚³æ‚¨çš„ PDF æˆç¸¾å–®æª”æ¡ˆï¼Œå·¥å…·å°‡å˜—è©¦æå–å…¶ä¸­çš„è¡¨æ ¼æ•¸æ“šä¸¦è¨ˆç®—ç¸½å­¸åˆ†ã€‚")

    uploaded_file = st.file_uploader("é¸æ“‡ä¸€å€‹ PDF æª”æ¡ˆ", type="pdf")

    if uploaded_file is not None:
        st.success(f"å·²ä¸Šå‚³æª”æ¡ˆ: **{uploaded_file.name}**")
        with st.spinner("æ­£åœ¨è™•ç† PDFï¼Œè«‹ç¨å€™..."):
            extracted_dfs = process_pdf_file(uploaded_file)

        if extracted_dfs:
            st.success("æˆåŠŸæå–æ‰€æœ‰è¡¨æ ¼æ•¸æ“šï¼")
            st.write("ä»¥ä¸‹æ˜¯æ‰€æœ‰æå–åˆ°çš„è¡¨æ ¼æ•¸æ“š (æ¯å€‹è¡¨æ ¼ä½œç‚ºä¸€å€‹ DataFrame)ï¼š")
            
            try:
                # å˜—è©¦å°‡æ‰€æœ‰ DataFrame åˆä½µï¼Œå¦‚æœæ¬„ä½åç¨±ä¸ä¸€è‡´ï¼Œæœƒå°è‡´ NaN
                combined_df = pd.concat(extracted_dfs, ignore_index=True)
                st.subheader("æ‰€æœ‰æ­·å¹´æˆç¸¾è¡¨æ ¼åˆä½µå¾Œçš„æ•¸æ“š (è‹¥çµæ§‹ç›¸å®¹)")
                st.dataframe(combined_df)
                
                # è¨ˆç®—ç¸½å­¸åˆ†
                total_credits = calculate_total_credits(extracted_dfs)
                st.markdown(f"## ç¸½è¨ˆå­¸åˆ†: **{total_credits:.2f}**")
                st.info("è«‹æ³¨æ„ï¼šå­¸åˆ†è¨ˆç®—æ˜¯åŸºæ–¼åµæ¸¬åˆ°çš„ã€Œå­¸åˆ†ã€æ¬„ä½åŠ ç¸½ï¼Œä¸¦æ’é™¤ã€ŒæŠµå…ã€ã€ã€Œé€šéã€ç­‰éæ•¸å­—æˆ–éæ­£æ•¸å­¸åˆ†ã€‚")

                # æä¾›ä¸‹è¼‰é¸é …
                csv_data = combined_df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ä¸‹è¼‰æ‰€æœ‰æ•¸æ“šç‚º CSV",
                    data=csv_data,
                    file_name=f"{uploaded_file.name.replace('.pdf', '')}_extracted_data.csv",
                    mime="text/csv",
                )
            except Exception as e_concat:
                st.warning(f"ç„¡æ³•å°‡æ‰€æœ‰æå–çš„è¡¨æ ¼åˆä½µï¼š`{e_concat}`ã€‚é€™é€šå¸¸æ˜¯å› ç‚ºä¸åŒè¡¨æ ¼çš„æ¬„ä½çµæ§‹æˆ–æ•¸é‡ä¸ä¸€è‡´ã€‚")
                st.info("æ¯å€‹å–®ç¨çš„è¡¨æ ¼å·²åœ¨ä¸Šæ–¹ç¨ç«‹é¡¯ç¤ºï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹å–®ç¨çš„è¡¨æ ¼çµæœã€‚")
                # å³ä½¿åˆä½µå¤±æ•—ï¼Œä¹Ÿå˜—è©¦è¨ˆç®—å­¸åˆ†
                total_credits = calculate_total_credits(extracted_dfs)
                st.markdown(f"## ç¸½è¨ˆå­¸åˆ†: **{total_credits:.2f}**")
                st.info("è«‹æ³¨æ„ï¼šå­¸åˆ†è¨ˆç®—æ˜¯åŸºæ–¼åµæ¸¬åˆ°çš„ã€Œå­¸åˆ†ã€æ¬„ä½åŠ ç¸½ï¼Œä¸¦æ’é™¤ã€ŒæŠµå…ã€ã€ã€Œé€šéã€ç­‰éæ•¸å­—æˆ–éæ­£æ•¸å­¸åˆ†ã€‚")
        else:
            st.warning("æœªå¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚è«‹æª¢æŸ¥ PDF å…§å®¹æˆ–å˜—è©¦èª¿æ•´ `table_settings`ã€‚")
    else:
        st.info("è«‹ä¸Šå‚³ PDF æª”æ¡ˆä»¥é–‹å§‹è™•ç†ã€‚")

if __name__ == "__main__":
    main()
