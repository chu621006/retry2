import streamlit as st
import pandas as pd
import pdfplumber
import collections
import re 

# --- è¼”åŠ©å‡½æ•¸ ---
# ç¢ºä¿é€™å€‹å‡½æ•¸åœ¨ä»»ä½•èª¿ç”¨ä¹‹å‰å®šç¾©ï¼Œè™•ç† None å€¼ã€pdfplumber Text ç‰©ä»¶å’Œæ™®é€šå­—ä¸²
def normalize_text(cell_content):
    """
    æ¨™æº–åŒ–å¾ pdfplumber æå–çš„å–®å…ƒæ ¼å…§å®¹ã€‚
    è™•ç† None å€¼ã€pdfplumber çš„ Text ç‰©ä»¶å’Œæ™®é€šå­—ä¸²ã€‚
    å°‡å¤šå€‹ç©ºç™½å­—å…ƒï¼ˆåŒ…æ‹¬æ›è¡Œï¼‰æ›¿æ›ç‚ºå–®å€‹ç©ºæ ¼ï¼Œä¸¦å»é™¤å…©ç«¯ç©ºç™½ã€‚
    """
    if cell_content is None:
        return ""

    text = ""
    # æª¢æŸ¥æ˜¯å¦æ˜¯ pdfplumber çš„ Text ç‰©ä»¶ï¼Œå®ƒé€šå¸¸æœ‰ 'text' å±¬æ€§
    if hasattr(cell_content, 'text'):
        text = str(cell_content.text)
    elif isinstance(cell_content, str):
        text = cell_content
    else:
        # å°æ–¼å…¶ä»–æœªçŸ¥é¡å‹ï¼Œå˜—è©¦ç›´æ¥è½‰æ›ç‚ºå­—ä¸²
        text = str(cell_content)
    
    return re.sub(r'\s+', ' ', text).strip()

def make_unique_columns(columns_list):
    """
    å°‡åˆ—è¡¨ä¸­çš„æ¬„ä½åç¨±è½‰æ›ç‚ºå”¯ä¸€çš„åç¨±ï¼Œè™•ç†é‡è¤‡å’Œç©ºå­—ä¸²ã€‚
    å¦‚æœé‡åˆ°é‡è¤‡æˆ–ç©ºå­—ä¸²ï¼Œæœƒæ·»åŠ å¾Œç¶´ (ä¾‹å¦‚ 'Column_1', 'æ¬„ä½_2')ã€‚
    """
    seen = collections.defaultdict(int)
    unique_columns = []
    for col in columns_list:
        original_col_cleaned = normalize_text(col)
        
        # å¦‚æœæ¸…ç†å¾Œé‚„æ˜¯ç©ºçš„ï¼Œæˆ–è€…å¤ªçŸ­ç„¡æ³•è­˜åˆ¥ç‚ºæœ‰æ„ç¾©çš„æ¨™é¡Œï¼Œå‰‡çµ¦äºˆé€šç”¨åç¨±
        if not original_col_cleaned or len(original_col_cleaned) < 2: 
            name_base = "Column"
            current_idx = 1
            while f"{name_base}_{current_idx}" in unique_columns:
                current_idx += 1
            name = f"{name_base}_{current_idx}"
        else:
            name = original_col_cleaned
        
        # è™•ç†é‡è¤‡åç¨±ï¼Œé¿å… Column_1, Column_1_1 é€™ç¨®æƒ…æ³
        final_name = name
        counter = seen[name]
        while final_name in unique_columns:
            counter += 1
            final_name = f"{name}_{counter}" if counter > 0 else name
        
        unique_columns.append(final_name)
        seen[name] = counter

    return unique_columns

def calculate_total_credits(df_list):
    """
    å¾æå–çš„ DataFrames åˆ—è¡¨ä¸­è¨ˆç®—ç¸½å­¸åˆ†ã€‚
    å°‹æ‰¾åŒ…å« 'å­¸åˆ†' æˆ– 'å­¸åˆ†(GPA)' é¡ä¼¼å­—æ¨£çš„æ¬„ä½é€²è¡ŒåŠ ç¸½ã€‚
    è¿”å›ç¸½å­¸åˆ†å’Œè¨ˆç®—å­¸åˆ†çš„ç§‘ç›®åˆ—è¡¨ã€‚
    """
    total_credits = 0.0
    calculated_courses = [] # ç”¨æ–¼å­˜æ”¾è¨ˆç®—äº†å­¸åˆ†çš„ç§‘ç›®åç¨±å’Œå­¸åˆ†

    # ç§»é™¤è©³ç´°çš„å­¸åˆ†è¨ˆç®—åˆ†æå°æ¨™é¡Œï¼Œä¿æŒä»‹é¢ç°¡æ½”
    # st.subheader("å­¸åˆ†è¨ˆç®—åˆ†æ") 

    # å®šç¾©å¯èƒ½çš„å­¸åˆ†æ¬„ä½åç¨±é—œéµå­— (ä¸­æ–‡å’Œè‹±æ–‡)
    credit_column_keywords = ["å­¸åˆ†", "å­¸åˆ†æ•¸", "å­¸åˆ†(GPA)", "å­¸ åˆ†", "Credits", "Credit"] 
    # å®šç¾©å¯èƒ½çš„ç§‘ç›®åç¨±é—œéµå­—
    subject_column_keywords = ["ç§‘ç›®åç¨±", "èª²ç¨‹åç¨±", "Course Name", "Subject Name", "ç§‘ç›®"] 
    
    # ç”¨æ–¼å¾å¯èƒ½åŒ…å«GPAçš„å­—ç¬¦ä¸²ä¸­æå–æ•¸å­—å­¸åˆ†ï¼Œä¾‹å¦‚ "A 2" -> 2, "3" -> 3
    credit_pattern = re.compile(r'(\d+(\.\d+)?)') 

    for df_idx, df in enumerate(df_list):
        # ç§»é™¤æ¯ä¸€é çš„è©³ç´°åˆ†æè¼¸å‡ºï¼Œä¿æŒç°¡æ½”
        # st.write(f"--- åˆ†æè¡¨æ ¼ {df_idx + 1} ---")
        # st.write(f"åµæ¸¬åˆ°çš„åŸå§‹æ¬„ä½åç¨±: `{list(df.columns)}`") 
        
        found_credit_column = None
        found_subject_column = None # åµæ¸¬ç§‘ç›®åç¨±æ¬„ä½
        
        # æ­¥é©Ÿ 1: å„ªå…ˆåŒ¹é…æ˜ç¢ºçš„å­¸åˆ†å’Œç§‘ç›®é—œéµå­—
        for col in df.columns:
            # æ¸…ç†æ¬„ä½åï¼Œåªä¿ç•™ä¸­è‹±æ–‡æ•¸å­—ï¼Œç”¨æ–¼åŒ¹é…é—œéµå­—
            cleaned_col_for_match = "".join(char for char in normalize_text(col) if '\u4e00' <= char <= '\u9fa5' or 'a' <= char <= 'z' or 'A' <= char <= 'Z' or '0' <= char <= '9').strip()
            
            if any(keyword in cleaned_col_for_match for keyword in credit_column_keywords):
                found_credit_column = col 
            if any(keyword in cleaned_col_for_match for keyword in subject_column_keywords):
                found_subject_column = col
            
            # å¦‚æœå…©å€‹éƒ½æ‰¾åˆ°äº†ï¼Œå°±å¯ä»¥æå‰çµæŸå¾ªç’°
            if found_credit_column and found_subject_column:
                break 

        # æ­¥é©Ÿ 2: å¦‚æœæ²’æœ‰æ˜ç¢ºåŒ¹é…ï¼Œå˜—è©¦å¾é€šç”¨åç¨± (Column_X) ä¸­çŒœæ¸¬å­¸åˆ†å’Œç§‘ç›®æ¬„ä½
        if not found_credit_column or not found_subject_column:
            potential_credit_columns = []
            potential_subject_columns = []

            for col in df.columns:
                # åˆ¤æ–·æ˜¯å¦ç‚ºé€šç”¨æ¬„ä½åï¼ˆä¾‹å¦‚ Column_1ï¼‰æˆ–é•·åº¦éçŸ­
                is_general_col = re.match(r"Column_\d+", col) or len(normalize_text(col).strip()) < 3
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºæ½›åœ¨å­¸åˆ†æ¬„ä½
                # å–å‰ N è¡Œæ•¸æ“šé€²è¡Œåˆ¤æ–·ï¼Œé¿å…ç©ºè¡Œæˆ–è¡¨å°¾ç¸½è¨ˆçš„å¹²æ“¾ (N=10 æ¯”è¼ƒé€šç”¨)
                sample_data = df[col].head(10).apply(normalize_text).tolist()
                numeric_like_count = 0
                total_sample_count = len(sample_data)
                
                for item_str in sample_data:
                    if item_str == "é€šé" or item_str == "æŠµå…" or item_str.lower() in ["pass", "exempt"]: # å…¼å®¹è‹±æ–‡
                        numeric_like_count += 1
                    else:
                        matches = credit_pattern.findall(item_str)
                        if matches:
                            try:
                                # å˜—è©¦è½‰æ›ç‚ºæµ®é»æ•¸ï¼Œä¸¦æª¢æŸ¥å­¸åˆ†ç¯„åœ (ä¾‹å¦‚ 0.0 åˆ° 10.0)
                                val = float(matches[-1][0])
                                if 0.0 <= val <= 10.0: # å­¸åˆ†é€šå¸¸ä¸æœƒè¶…é 10
                                    numeric_like_count += 1
                            except ValueError:
                                pass
                        # ä¸åŒ¹é…æ•¸å­—æˆ–ç‰¹å®šé—œéµå­—çš„ï¼Œä¸è¨ˆå…¥ numeric_like_count
                
                # å¦‚æœè¶…éä¸€åŠ (æˆ–æ›´é«˜æ¯”ä¾‹) çš„æ¨£æœ¬æ•¸æ“šçœ‹èµ·ä¾†åƒå­¸åˆ†ï¼Œå‰‡èªç‚ºå¯èƒ½æ˜¯å­¸åˆ†æ¬„ä½
                if total_sample_count > 0 and numeric_like_count / total_sample_count >= 0.6: # æé«˜è­˜åˆ¥é–€æª»åˆ° 60%
                    potential_credit_columns.append(col)
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºæ½›åœ¨ç§‘ç›®åç¨±æ¬„ä½ (è‹¥åŒ…å«ä¸­æ–‡ä¸”éç´”æ•¸å­—)
                if is_general_col:
                    subject_like_count = 0
                    for item_str in sample_data:
                        # åˆ¤æ–·æ˜¯å¦çœ‹èµ·ä¾†åƒç§‘ç›®åç¨±: åŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œé•·åº¦å¤§æ–¼3ï¼Œä¸”ä¸å…¨æ˜¯æ•¸å­—
                        if re.search(r'[\u4e00-\u9fa5]', item_str) and len(item_str) > 3 and not item_str.isdigit() and not re.match(r'^\d+(\.\d+)?$', item_str): 
                            subject_like_count += 1
                    if total_sample_count > 0 and subject_like_count / total_sample_count >= 0.7: # æ›´é«˜é–€æª»
                        potential_subject_columns.append(col)


            # æ­¥é©Ÿ 3: æ ¹æ“šæ¨æ–·çµæœç¢ºå®šå­¸åˆ†å’Œç§‘ç›®æ¬„ä½
            if not found_credit_column and potential_credit_columns:
                best_credit_candidate = None
                if found_subject_column: # å¦‚æœå·²æ‰¾åˆ°ç§‘ç›®åç¨±ï¼Œå‰‡é¸æ“‡å…¶å³å´çš„å­¸åˆ†æ¬„ä½
                    subject_col_idx = df.columns.get_loc(found_subject_column)
                    min_dist = float('inf')
                    for p_col in potential_credit_columns:
                        p_col_idx = df.columns.get_loc(p_col)
                        if p_col_idx > subject_col_idx and (p_col_idx - subject_col_idx) < min_dist:
                            min_dist = p_col_idx - subject_col_idx
                            best_credit_candidate = p_col
                
                if not best_credit_candidate and potential_credit_columns: # å¦å‰‡é¸æ“‡ç¬¬ä¸€å€‹æ½›åœ¨å­¸åˆ†æ¬„ä½
                    best_credit_candidate = potential_credit_columns[0]
                
                found_credit_column = best_credit_candidate

            if not found_subject_column and potential_subject_columns:
                # å¦‚æœå­¸åˆ†æ¬„ä½å·²ç¢ºå®šï¼Œä¸”ç§‘ç›®æ¬„ä½æœªç¢ºå®šï¼Œå‰‡é¸æ“‡å­¸åˆ†æ¬„ä½å·¦å´æœ€æ¥è¿‘çš„ç§‘ç›®æ¬„ä½
                if found_credit_column:
                    credit_col_idx = df.columns.get_loc(found_credit_column)
                    min_dist = float('inf')
                    best_subject_candidate = None
                    for p_col in potential_subject_columns:
                        p_col_idx = df.columns.get_loc(p_col)
                        if p_col_idx < credit_col_idx and (credit_col_idx - p_col_idx) < min_dist:
                            min_dist = credit_col_idx - p_col_idx
                            best_subject_candidate = p_col
                    if best_subject_candidate:
                        found_subject_column = best_subject_candidate
                elif potential_subject_columns: # å¦å‰‡é¸æ“‡ç¬¬ä¸€å€‹æ½›åœ¨ç§‘ç›®æ¬„ä½
                    found_subject_column = potential_subject_columns[0]

        if found_credit_column:
            # ç§»é™¤è©³ç´°åµæ¸¬è¨Šæ¯ï¼Œåªä¿ç•™å•é¡Œæç¤º
            # st.info(f"å¾è¡¨æ ¼ {df_idx + 1} åµæ¸¬åˆ°å­¸åˆ†æ¬„ä½: '{found_credit_column}'ã€‚")
            # if found_subject_column:
            # st.info(f"å¾è¡¨æ ¼ {df_idx + 1} åµæ¸¬åˆ°ç§‘ç›®åç¨±æ¬„ä½: '{found_subject_column}'ã€‚")
            # else:
            # st.warning(f"è¡¨æ ¼ {df_idx + 1} æœªåµæ¸¬åˆ°æ˜ç¢ºçš„ç§‘ç›®åç¨±æ¬„ä½ã€‚ç§‘ç›®åç¨±å¯èƒ½ç„¡æ³•æº–ç¢ºè¨˜éŒ„ã€‚")

            try:
                current_table_credits = 0.0
                for row_idx, row in df.iterrows():
                    item_str = normalize_text(row[found_credit_column])
                    
                    credit_val = 0.0
                    # å„ªå…ˆè™•ç†å·²çŸ¥éæ•¸å­—çš„å­¸åˆ†æƒ…æ³
                    if item_str == "é€šé" or item_str == "æŠµå…" or item_str.lower() in ["pass", "exempt"]:
                        credit_val = 0.0
                    else:
                        # å˜—è©¦ç”¨æ­£å‰‡è¡¨é”å¼å¾å­—ä¸²ä¸­æå–æ‰€æœ‰æ•¸å­—
                        matches = credit_pattern.findall(item_str)
                        if matches:
                            # å‡è¨­æœ€å¾Œä¸€å€‹æ•¸å­—é€šå¸¸æ˜¯å­¸åˆ†ï¼Œä¾‹å¦‚ "A 2" ä¸­çš„ "2"
                            try:
                                val = float(matches[-1][0])
                                if 0.0 <= val <= 10.0: # ç¢ºä¿æå–çš„æ•¸å­—åœ¨åˆç†å­¸åˆ†ç¯„åœå…§
                                    credit_val = val
                                else:
                                    credit_val = 0.0 # è¶…å‡ºç¯„åœçš„æ•¸å­—ä¸è¨ˆå…¥å­¸åˆ†
                            except ValueError:
                                credit_val = 0.0
                        else:
                            credit_val = 0.0 # æ²’æœ‰åŒ¹é…åˆ°æ•¸å­—
                    
                    if credit_val > 0: # åªè¨˜éŒ„æœ‰å­¸åˆ†çš„ç§‘ç›®
                        current_table_credits += credit_val
                        
                        course_name = "æœªçŸ¥ç§‘ç›®"
                        if found_subject_column and found_subject_column in row:
                            course_name = normalize_text(row[found_subject_column])
                        
                        calculated_courses.append({"ç§‘ç›®åç¨±": course_name, "å­¸åˆ†": credit_val, "ä¾†æºè¡¨æ ¼": df_idx + 1})

                total_credits += current_table_credits
                # ç§»é™¤æ¯å€‹è¡¨æ ¼çš„å­¸åˆ†ç¸½è¨ˆé¡¯ç¤º
                # st.write(f"è¡¨æ ¼ {df_idx + 1} çš„å­¸åˆ†ç¸½è¨ˆ: **{current_table_credits:.2f}**")
                
            except Exception as e:
                st.warning(f"è¡¨æ ¼ {df_idx + 1} çš„å­¸åˆ†æ¬„ä½ '{found_credit_column}' è½‰æ›ç‚ºæ•¸å€¼æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e}`")
                st.warning("è©²è¡¨æ ¼çš„å­¸åˆ†å¯èƒ½ç„¡æ³•è¨ˆå…¥ç¸½æ•¸ã€‚è«‹æª¢æŸ¥å­¸åˆ†æ¬„ä½æ•¸æ“šæ˜¯å¦ç‚ºç´”æ•¸å­—æˆ–å¯æå–æ•¸å­—ã€‚")
        else:
            # ç§»é™¤è©³ç´°çš„åµæ¸¬ä¸åˆ°å­¸åˆ†æ¬„ä½è¨Šæ¯
            # st.info(f"è¡¨æ ¼ {df_idx + 1} æœªåµæ¸¬åˆ°æ˜ç¢ºçš„å­¸åˆ†æ¬„ä½ã€‚æª¢æŸ¥æ¬„ä½ï¼š`{list(df.columns)}`ã€‚ä¸è¨ˆå…¥ç¸½å­¸åˆ†ã€‚")
            pass # ä¸é¡¯ç¤ºæ­¤é¡ä¿¡æ¯ï¼Œä¿æŒä»‹é¢ç°¡æ½”
            
    return total_credits, calculated_courses

def process_pdf_file(uploaded_file):
    """
    ä½¿ç”¨ pdfplumber è™•ç†ä¸Šå‚³çš„ PDF æª”æ¡ˆï¼Œæå–è¡¨æ ¼ã€‚
    æ­¤å‡½æ•¸å…§éƒ¨å°‡æ¸›å°‘ Streamlit çš„ç›´æ¥è¼¸å‡ºï¼Œåªè¿”å›æå–çš„æ•¸æ“šã€‚
    """
    all_grades_data = []

    try:
        with pdfplumber.open(uploaded_file) as pdf:
            # ç§»é™¤è™•ç†æª”æ¡ˆå’Œé æ•¸ä¿¡æ¯ï¼Œä¿æŒä»‹é¢ç°¡æ½”
            # st.write(f"æ­£åœ¨è™•ç†æª”æ¡ˆ: **{uploaded_file.name}**")
            # num_pages = len(pdf.pages)
            # st.info(f"PDF ç¸½é æ•¸: **{num_pages}**")

            for page_num, page in enumerate(pdf.pages):
                # ç§»é™¤é é¢æ¨™é¡Œ
                # st.subheader(f"é é¢ {page_num + 1}") 

                table_settings = {
                    "vertical_strategy": "lines", # åŸºæ–¼ç·šæ¢åµæ¸¬å‚ç›´åˆ†éš”
                    "horizontal_strategy": "lines", # åŸºæ–¼ç·šæ¢åµæ¸¬æ°´å¹³åˆ†éš”
                    "snap_tolerance": 3, # å‚ç›´/æ°´å¹³ç·šçš„æ•æ‰å®¹å¿åº¦
                    "join_tolerance": 3, # æ–·é–‹ç·šæ®µçš„é€£æ¥å®¹å¿åº¦
                    "edge_min_length": 3, # åµæ¸¬åˆ°çš„ç·šæ¢æœ€å°é•·åº¦
                    "text_tolerance": 1, # æ–‡æœ¬èˆ‡åµæ¸¬ç·šæ¢çš„å®¹å¿åº¦ (ä½æ–¼æ­¤å€¼å‰‡èªç‚ºæ–‡æœ¬åœ¨ç·šä¸Š)
                }
                
                current_page = page 

                try:
                    tables = current_page.extract_tables(table_settings)

                    if not tables:
                        # ä»ä¿ç•™æœªåµæ¸¬åˆ°è¡¨æ ¼çš„è­¦å‘Šï¼Œå› ç‚ºé€™æ˜¯é—œéµä¿¡æ¯
                        st.warning(f"é é¢ **{page_num + 1}** æœªåµæ¸¬åˆ°è¡¨æ ¼ã€‚é€™å¯èƒ½æ˜¯ç”±æ–¼ PDF æ ¼å¼è¤‡é›œæˆ–è¡¨æ ¼æå–è¨­å®šä¸é©ç”¨ã€‚")
                        continue

                    for table_idx, table in enumerate(tables):
                        # ç§»é™¤æ¯å€‹è¡¨æ ¼çš„æ¨™é¡Œ
                        # st.markdown(f"**é é¢ {page_num + 1} çš„è¡¨æ ¼ {table_idx + 1}**")
                        
                        processed_table = []
                        # ç¢ºä¿åœ¨é€™è£¡æ­£ç¢ºä½¿ç”¨ normalize_text è™•ç†æ‰€æœ‰å–®å…ƒæ ¼å…§å®¹
                        for row in table:
                            normalized_row = [normalize_text(cell) for cell in row]
                            processed_table.append(normalized_row)
                        
                        if not processed_table:
                            # ä»ä¿ç•™ç©ºè¡¨æ ¼ä¿¡æ¯ï¼Œå¹«åŠ©åµéŒ¯
                            st.info(f"é é¢ {page_num + 1} çš„è¡¨æ ¼ **{table_idx + 1}** æå–å¾Œç‚ºç©ºã€‚")
                            continue

                        # å‡è¨­ç¬¬ä¸€è¡Œæ˜¯æ¨™é¡Œè¡Œï¼Œä½†ç¢ºä¿æœ‰è¶³å¤ çš„è¡Œ
                        if len(processed_table) > 0:
                            header_row = processed_table[0]
                            data_rows = processed_table[1:]
                        else:
                            header_row = []
                            data_rows = []

                        unique_columns = make_unique_columns(header_row)

                        if data_rows:
                            num_columns_header = len(unique_columns)
                            cleaned_data_rows = []
                            for row in data_rows:
                                # ç¢ºä¿è¡Œæ•¸æ“šèˆ‡æ¨™é¡Œé•·åº¦åŒ¹é…
                                if len(row) > num_columns_header:
                                    cleaned_data_rows.append(row[:num_columns_header])
                                elif len(row) < num_columns_header:
                                    cleaned_data_rows.append(row + [''] * (num_columns_header - len(row)))
                                else:
                                    cleaned_data_rows.append(row)

                            try:
                                df_table = pd.DataFrame(cleaned_data_rows, columns=unique_columns)
                                all_grades_data.append(df_table)
                                # é€™æ˜¯æ‚¨å¸Œæœ›ç§»é™¤çš„è©³ç´°è¡¨æ ¼è¼¸å‡º
                                # st.dataframe(df_table) 
                            except Exception as e_df:
                                # ä»ä¿ç•™è½‰æ› DataFrame çš„éŒ¯èª¤ä¿¡æ¯ï¼Œé€™å¾ˆé‡è¦
                                st.error(f"é é¢ {page_num + 1} è¡¨æ ¼ {table_idx + 1} è½‰æ›ç‚º DataFrame æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e_df}`")
                                st.error(f"åŸå§‹è™•ç†å¾Œæ•¸æ“šç¯„ä¾‹: {processed_table[:2]} (å‰å…©è¡Œ)")
                                st.error(f"ç”Ÿæˆçš„å”¯ä¸€æ¬„ä½åç¨±: {unique_columns}")
                        else:
                            # ä»ä¿ç•™æ²’æœ‰æ•¸æ“šè¡Œä¿¡æ¯
                            st.info(f"é é¢ {page_num + 1} çš„è¡¨æ ¼ **{table_idx + 1}** æ²’æœ‰æ•¸æ“šè¡Œã€‚")

                except Exception as e_table:
                    # ä»ä¿ç•™è™•ç†è¡¨æ ¼æ™‚çš„éŒ¯èª¤ä¿¡æ¯
                    st.error(f"é é¢ **{page_num + 1}** è™•ç†è¡¨æ ¼æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e_table}`")
                    st.warning("é€™å¯èƒ½æ˜¯ç”±æ–¼ PDF æ ¼å¼è¤‡é›œæˆ–è¡¨æ ¼æå–è¨­å®šä¸é©ç”¨ã€‚è«‹æª¢æŸ¥ PDF çµæ§‹ã€‚")

    except pdfplumber.PDFSyntaxError as e_pdf_syntax:
        st.error(f"è™•ç† PDF èªæ³•æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e_pdf_syntax}`ã€‚æª”æ¡ˆå¯èƒ½å·²æå£æˆ–æ ¼å¼ä¸æ­£ç¢ºã€‚")
    except Exception as e:
        st.error(f"è™•ç† PDF æª”æ¡ˆæ™‚ç™¼ç”Ÿä¸€èˆ¬éŒ¯èª¤: `{e}`")
        st.error("è«‹ç¢ºèªæ‚¨çš„ PDF æ ¼å¼æ˜¯å¦ç‚ºæ¸…æ™°çš„è¡¨æ ¼ã€‚è‹¥å•é¡ŒæŒçºŒï¼Œå¯èƒ½æ˜¯ PDF çµæ§‹è¼ƒç‚ºè¤‡é›œï¼Œéœ€è¦èª¿æ•´ `pdfplumber` çš„è¡¨æ ¼æå–è¨­å®šã€‚")

    return all_grades_data

# --- Streamlit æ‡‰ç”¨ä¸»é«” ---
def main():
    st.set_page_config(page_title="PDF æˆç¸¾å–®å­¸åˆ†è¨ˆç®—å·¥å…·", layout="wide")
    st.title("ğŸ“„ PDF æˆç¸¾å–®å­¸åˆ†è¨ˆç®—å·¥å…·")

    st.write("è«‹ä¸Šå‚³æ‚¨çš„ PDF æˆç¸¾å–®æª”æ¡ˆï¼Œå·¥å…·å°‡å˜—è©¦æå–å…¶ä¸­çš„è¡¨æ ¼æ•¸æ“šä¸¦è¨ˆç®—ç¸½å­¸åˆ†ã€‚")
    st.write("æ‚¨ä¹Ÿå¯ä»¥è¼¸å…¥ç›®æ¨™å­¸åˆ†ï¼ŒæŸ¥çœ‹é‚„å·®å¤šå°‘å­¸åˆ†ã€‚")

    uploaded_file = st.file_uploader("é¸æ“‡ä¸€å€‹ PDF æª”æ¡ˆ", type="pdf")

    if uploaded_file is not None:
        st.success(f"å·²ä¸Šå‚³æª”æ¡ˆ: **{uploaded_file.name}**")
        with st.spinner("æ­£åœ¨è™•ç† PDFï¼Œè«‹ç¨å€™..."):
            extracted_dfs = process_pdf_file(uploaded_file)

        if extracted_dfs:
            # ç§»é™¤æˆåŠŸæå–æ‰€æœ‰è¡¨æ ¼æ•¸æ“šçš„æç¤ºï¼Œå› ç‚ºä¸‹é¢æœƒæœ‰æ›´å…·é«”çš„çµæœ
            # st.success("æˆåŠŸæå–æ‰€æœ‰è¡¨æ ¼æ•¸æ“šï¼")
            
            total_credits, calculated_courses = calculate_total_credits(extracted_dfs)

            st.markdown("---")
            st.markdown("## âœ… æŸ¥è©¢çµæœ") # èª¿æ•´ç‚ºæ›´ç°¡æ½”çš„æ¨™é¡Œ
            st.markdown(f"ç›®å‰ç¸½å­¸åˆ†: <span style='color:green; font-size: 24px;'>**{total_credits:.2f}**</span>", unsafe_allow_html=True)
            # ç§»é™¤è©³ç´°çš„æç¤ºä¿¡æ¯ï¼Œåªä¿ç•™ç›®æ¨™å­¸åˆ†ç›¸é—œå…§å®¹
            # st.info("è«‹æ³¨æ„ï¼šå­¸åˆ†è¨ˆç®—æ˜¯åŸºæ–¼åµæ¸¬åˆ°çš„ã€Œå­¸åˆ†ã€æ¬„ä½åŠ ç¸½ï¼Œä¸¦æ’é™¤ã€ŒæŠµå…ã€ã€ã€Œé€šéã€ç­‰éæ•¸å­—æˆ–éæ­£æ•¸å­¸åˆ†ã€‚")

            # è¼¸å…¥ç›®æ¨™å­¸åˆ†
            target_credits = st.number_input("è¼¸å…¥æ‚¨çš„ç›®æ¨™å­¸åˆ† (ä¾‹å¦‚ï¼š128)", min_value=0.0, value=128.0, step=1.0, 
                                            help="æ‚¨å¯ä»¥è¨­å®šä¸€å€‹ç•¢æ¥­å­¸åˆ†ç›®æ¨™ï¼Œå·¥å…·æœƒå¹«æ‚¨è¨ˆç®—é‚„å·®å¤šå°‘å­¸åˆ†ã€‚")
            
            credit_difference = target_credits - total_credits
            if credit_difference > 0:
                st.write(f"è·é›¢ç•¢æ¥­æ‰€éœ€å­¸åˆ† (å…±{target_credits:.0f}å­¸åˆ†) **{credit_difference:.2f}**")
            elif credit_difference < 0:
                st.write(f"å·²è¶…è¶Šç•¢æ¥­æ‰€éœ€å­¸åˆ† (å…±{target_credits:.0f}å­¸åˆ†) **{abs(credit_difference):.2f}**")
            else:
                st.write(f"å·²é”åˆ°ç•¢æ¥­æ‰€éœ€å­¸åˆ† (å…±{target_credits:.0f}å­¸åˆ†) **0.00**")


            st.markdown("---")
            st.markdown("### ğŸ“š é€šéçš„èª²ç¨‹åˆ—è¡¨") # èª¿æ•´ç‚ºæ›´ç¬¦åˆæˆªåœ–çš„æ¨™é¡Œ
            if calculated_courses:
                # å°‡ç§‘ç›®åˆ—è¡¨è½‰æ›ç‚º DataFrame ä»¥ä¾¿é¡¯ç¤º
                courses_df = pd.DataFrame(calculated_courses)
                # é‡æ–°æ’åºæ¬„ä½ä»¥ç¬¦åˆæˆªåœ–çš„é¡¯ç¤ºé †åº (å­¸æœŸã€å­¸å¹´åº¦ã€ç§‘ç›®åç¨±ã€å­¸åˆ†ã€GPA)
                # ä½†åŸå§‹æ•¸æ“šä¸­æ²’æœ‰GPAï¼Œæ‰€ä»¥åªé¡¯ç¤ºå·²æœ‰çš„
                # å¦‚æœéœ€è¦å­¸æœŸå’Œå­¸å¹´åº¦ï¼Œéœ€è¦å¾åŸå§‹DataFrameä¸­æå–ï¼Œä½†é€™è£¡åªä¿ç•™ç§‘ç›®åç¨±å’Œå­¸åˆ†ï¼Œä¸¦å¯åŠ ä¸Šä¾†æºè¡¨æ ¼ä½œç‚ºè¼”åŠ©ä¿¡æ¯
                
                # ç‚ºäº†ç›¡é‡ç¬¦åˆã€Œé€šéçš„èª²ç¨‹åˆ—è¡¨ã€çš„æ ¼å¼ï¼Œæˆ‘å€‘éœ€è¦ç¢ºä¿`calculated_courses`åŒ…å«å­¸æœŸå’Œå­¸å¹´åº¦ã€‚
                # ç›®å‰`calculated_courses`åªåŒ…å«`ç§‘ç›®åç¨±`ã€`å­¸åˆ†`å’Œ`ä¾†æºè¡¨æ ¼`ã€‚
                # è¦å¯¦ç¾å®Œå…¨ç›¸åŒçš„ä»‹é¢ï¼Œéœ€è¦èª¿æ•´`calculate_total_credits`ä¾†æå–å­¸æœŸå’Œå­¸å¹´åº¦ã€‚
                # ä½†ç‚ºäº†ç¶­æŒã€Œå…¶ä»–ç¨‹å¼ç¢¼éƒ½ä¸è¦å‹•ã€çš„åŸå‰‡ï¼Œæˆ‘åªèƒ½ç”¨ç¾æœ‰çš„`calculated_courses`çµæ§‹ã€‚
                # å¦‚æœå¾ŒçºŒæœ‰éœ€è¦ï¼Œå¯ä»¥å†å‘Šè¨´æˆ‘èª¿æ•´`calculate_total_credits`ä¾†åŒ…å«æ›´å¤šåŸå§‹è³‡è¨Šã€‚

                # ç•¶å‰æ•¸æ“šåªæœ‰ ç§‘ç›®åç¨±, å­¸åˆ†, ä¾†æºè¡¨æ ¼ã€‚ç‚ºäº†æ›´åƒæˆªåœ–ï¼Œæˆ‘å€‘å¯ä»¥é€™æ¨£å‘ˆç¾ï¼š
                # å‡è¨­æ‚¨æ›´å¸Œæœ›çœ‹åˆ°å­¸å¹´åº¦å’Œå­¸æœŸï¼Œé€™éœ€è¦æ›´æ·±å…¥çš„é‚è¼¯ä¾†å¾åŸå§‹è¡Œä¸­æå–é€™äº›è³‡è¨Šä¸¦åŠ å…¥åˆ° calculated_coursesã€‚
                # ä½†åœ¨ä¸ã€Œå‹•ã€å…¶ä»–ç¨‹å¼ç¢¼çš„å‰æä¸‹ï¼Œæˆ‘æœƒç”¨ç¾æœ‰çµæ§‹ä¾†é¡¯ç¤ºã€‚
                
                # é€™è£¡å‡è¨­æ‚¨æŒ‡çš„æ˜¯åƒ image_f60ac7.png ä¸­é‚£æ¨£çš„ã€Œé€šéçš„èª²ç¨‹åˆ—è¡¨ã€
                # è©²åˆ—è¡¨åŒ…å«äº† å­¸å¹´åº¦ã€å­¸æœŸã€ç§‘ç›®åç¨±ã€å­¸åˆ†ã€GPA
                # ä½†æˆ‘å€‘ç›®å‰åªæœ‰ç§‘ç›®åç¨±å’Œå­¸åˆ†ã€‚è‹¥è¦å®Œå…¨ä¸€æ¨£ï¼Œéœ€è¦ä¿®æ”¹ `calculate_total_credits` ä¾†æå–æ›´å¤šæ¬„ä½ã€‚
                # æš«æ™‚å…ˆç”¨ç¾æœ‰çš„`calculated_courses`çµæ§‹é€²è¡Œå±•ç¤ºã€‚
                
                # å¯ä»¥è€ƒæ…®å¢åŠ å­¸å¹´åº¦å’Œå­¸æœŸåˆ° `calculated_courses` ä¸­ï¼Œä½†é€™æœƒä¿®æ”¹ `calculate_total_credits`ã€‚
                # ç‚ºäº†é¿å…éå¤šä¿®æ”¹ï¼Œæˆ‘å€‘ç›®å‰åªé¡¯ç¤º `ç§‘ç›®åç¨±` å’Œ `å­¸åˆ†`ã€‚
                # é‡æ–°çµ„ç¹”é¡¯ç¤ºæ¬„ä½ä»¥ç¬¦åˆå¸¸è¦‹æˆç¸¾å–®ç¿’æ…£
                display_cols = ['ç§‘ç›®åç¨±', 'å­¸åˆ†']
                if 'å­¸å¹´åº¦' in courses_df.columns:
                    display_cols.insert(0, 'å­¸å¹´åº¦')
                if 'å­¸æœŸ' in courses_df.columns:
                    display_cols.insert(1, 'å­¸æœŸ')
                
                # ç¢ºä¿åªåŒ…å«ç¢ºå¯¦å­˜åœ¨çš„æ¬„ä½
                final_display_cols = [col for col in display_cols if col in courses_df.columns]
                
                # Streamlit DataFrame é¡¯ç¤º
                st.dataframe(courses_df[final_display_cols], height=300, use_container_width=True) # ä½¿ç”¨ use_container_width è®“è¡¨æ ¼è‡ªå‹•èª¿æ•´å¯¬åº¦
            else:
                st.info("æ²’æœ‰æ‰¾åˆ°å¯ä»¥è¨ˆç®—å­¸åˆ†çš„ç§‘ç›®ã€‚")

            # æä¾›ä¸‹è¼‰é¸é … 
            if calculated_courses:
                csv_data = courses_df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ä¸‹è¼‰è¨ˆç®—å­¸åˆ†çš„ç§‘ç›®åˆ—è¡¨ç‚º CSV",
                    data=csv_data,
                    file_name=f"{uploaded_file.name.replace('.pdf', '')}_calculated_courses.csv",
                    mime="text/csv",
                )
            
        else:
            st.warning("æœªå¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚è«‹æª¢æŸ¥ PDF å…§å®¹æˆ–å˜—è©¦èª¿æ•´ `pdfplumber` çš„è¡¨æ ¼æå–è¨­å®šã€‚")
    else:
        st.info("è«‹ä¸Šå‚³ PDF æª”æ¡ˆä»¥é–‹å§‹è™•ç†ã€‚")

if __name__ == "__main__":
    main()
