import streamlit as st
import pandas as pd
import pdfplumber

# --- è¼”åŠ©å‡½æ•¸ ---
def normalize_text(cell_content):
    """
    æ¨™æº–åŒ–å¾ pdfplumber æå–çš„å–®å…ƒæ ¼å…§å®¹ã€‚
    è™•ç† None å€¼ã€pdfplumber çš„ Text ç‰©ä»¶å’Œæ™®é€šå­—ä¸²ã€‚
    """
    if cell_content is None:
        return ""  # è¿”å›ç©ºå­—ä¸²ä¾†è¡¨ç¤ºç©ºç™½å–®å…ƒæ ¼

    # æª¢æŸ¥æ˜¯å¦ç‚º pdfplumber çš„ Text ç‰©ä»¶ (é€šå¸¸æœ‰ .text å±¬æ€§)
    if hasattr(cell_content, 'text'):
        return str(cell_content.text).strip()
    # å¦‚æœå·²ç¶“æ˜¯å­—ä¸²
    elif isinstance(cell_content, str):
        return cell_content.strip()
    else:
        # å°æ–¼å…¶ä»–æœªçŸ¥é¡å‹ï¼Œå˜—è©¦è½‰æ›ç‚ºå­—ä¸²ä¸¦å»é™¤ç©ºç™½
        return str(cell_content).strip()

def process_pdf_file(uploaded_file):
    """
    ä½¿ç”¨ pdfplumber è™•ç†ä¸Šå‚³çš„ PDF æª”æ¡ˆï¼Œæå–è¡¨æ ¼ã€‚
    """
    all_grades_data = [] # åˆå§‹åŒ–ç”¨æ–¼å„²å­˜æ‰€æœ‰è¡¨æ ¼æ•¸æ“šçš„åˆ—è¡¨

    try:
        with pdfplumber.open(uploaded_file) as pdf:
            st.write(f"æ­£åœ¨è™•ç†æª”æ¡ˆ: **{uploaded_file.name}**")
            num_pages = len(pdf.pages)
            st.info(f"PDF ç¸½é æ•¸: **{num_pages}**")

            for page_num, page in enumerate(pdf.pages):
                st.subheader(f"é é¢ {page_num + 1}")

                # é€™è£¡ä½ å¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´ table_settings
                # å°æ–¼æˆç¸¾å–®é€™é¡æœ‰æ¸…æ™°ç·šæ¢çš„è¡¨æ ¼ï¼Œ'lines' ç­–ç•¥é€šå¸¸æ•ˆæœä¸éŒ¯ã€‚
                # å¦‚æœä»æœ‰å•é¡Œï¼Œå¯ä»¥å˜—è©¦èª¿æ•´å®¹å¿åº¦æˆ–å°‡ç­–ç•¥æ”¹ç‚º 'text'ã€‚
                table_settings = {
                    "vertical_strategy": "lines",
                    "horizontal_strategy": "lines",
                    "snap_tolerance": 3,           # èª¿æ•´ç·šæ¢æ•æ‰çš„å®¹å¿åº¦
                    "join_tolerance": 3,           # èª¿æ•´åˆä½µç·šæ¢çš„å®¹å¿åº¦
                    "edge_min_length": 3,          # æœ€å°é‚Šç·£é•·åº¦ï¼Œé¿å…åµæ¸¬åˆ°éçŸ­çš„ç·šæ¢
                    "text_tolerance": 1,           # æ–‡æœ¬æ¥è¿‘ç·šæ¢çš„å®¹å¿åº¦
                    # å¦‚æœè¡¨æ ¼ç·šæ¢ä¸æ˜é¡¯ï¼Œå¯ä»¥å˜—è©¦ï¼š
                    # "vertical_strategy": "text",
                    # "horizontal_strategy": "text",
                }

                try:
                    # å˜—è©¦æå–ç•¶å‰é é¢ä¸Šçš„æ‰€æœ‰è¡¨æ ¼
                    tables = page.extract_tables(table_settings)

                    if not tables:
                        st.warning(f"é é¢ **{page_num + 1}** æœªåµæ¸¬åˆ°è¡¨æ ¼ã€‚é€™å¯èƒ½æ˜¯ç”±æ–¼ PDF æ ¼å¼è¤‡é›œæˆ–è¡¨æ ¼æå–è¨­å®šä¸é©ç”¨ã€‚")
                        continue

                    for table_idx, table in enumerate(tables):
                        st.markdown(f"**é é¢ {page_num + 1} çš„è¡¨æ ¼ {table_idx + 1}**")
                        
                        # ä½¿ç”¨ normalize_text å‡½æ•¸è™•ç†æ¯å€‹å–®å…ƒæ ¼
                        processed_table = []
                        for row in table:
                            normalized_row = [normalize_text(cell) for cell in row]
                            processed_table.append(normalized_row)
                        
                        # å°‡è™•ç†å¾Œçš„è¡¨æ ¼è½‰æ›ç‚º DataFrame
                        if processed_table and len(processed_table) > 1:
                            # å‡è¨­ç¬¬ä¸€è¡Œæ˜¯æ¨™é¡Œ
                            df_table = pd.DataFrame(processed_table[1:], columns=processed_table[0])
                            all_grades_data.append(df_table)
                            st.dataframe(df_table)
                        elif processed_table:
                             # å¦‚æœåªæœ‰ä¸€è¡Œæˆ–æ²’æœ‰æ˜ç¢ºæ¨™é¡Œï¼Œç›´æ¥ä½œç‚ºæ•¸æ“š
                             df_table = pd.DataFrame(processed_table)
                             all_grades_data.append(df_table)
                             st.dataframe(df_table)
                        else:
                            st.info(f"è¡¨æ ¼ **{table_idx + 1}** æå–å¾Œç‚ºç©ºã€‚")

                except Exception as e_table:
                    st.error(f"é é¢ **{page_num + 1}** è™•ç†è¡¨æ ¼æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e_table}`")
                    st.warning("é€™å¯èƒ½æ˜¯ç”±æ–¼ PDF æ ¼å¼è¤‡é›œæˆ–è¡¨æ ¼æå–è¨­å®šä¸é©ç”¨ã€‚")

    except pdfplumber.PDFSyntaxError as e_pdf_syntax:
        st.error(f"è™•ç† PDF èªæ³•æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e_pdf_syntax}`ã€‚æª”æ¡ˆå¯èƒ½å·²æå£æˆ–æ ¼å¼ä¸æ­£ç¢ºã€‚")
    except Exception as e:
        st.error(f"è™•ç† PDF æª”æ¡ˆæ™‚ç™¼ç”Ÿä¸€èˆ¬éŒ¯èª¤: `{e}`")
        st.error("è«‹ç¢ºèªæ‚¨çš„ PDF æ ¼å¼æ˜¯å¦ç‚ºæ¸…æ™°çš„è¡¨æ ¼ã€‚è‹¥å•é¡ŒæŒçºŒï¼Œå¯èƒ½æ˜¯ PDF çµæ§‹è¼ƒç‚ºè¤‡é›œï¼Œéœ€è¦èª¿æ•´ `pdfplumber` çš„è¡¨æ ¼æå–è¨­å®šã€‚")

    return all_grades_data

# --- Streamlit æ‡‰ç”¨ä¸»é«” ---
def main():
    st.set_page_config(page_title="PDF æˆç¸¾å–®æå–å·¥å…·", layout="wide")
    st.title("ğŸ“„ PDF æˆç¸¾å–®è¡¨æ ¼æ•¸æ“šæå–")

    st.write("è«‹ä¸Šå‚³æ‚¨çš„ PDF æˆç¸¾å–®æª”æ¡ˆï¼Œå·¥å…·å°‡å˜—è©¦æå–å…¶ä¸­çš„è¡¨æ ¼æ•¸æ“šã€‚")

    uploaded_file = st.file_uploader("é¸æ“‡ä¸€å€‹ PDF æª”æ¡ˆ", type="pdf")

    if uploaded_file is not None:
        st.success(f"å·²ä¸Šå‚³æª”æ¡ˆ: **{uploaded_file.name}**")
        st.spinner("æ­£åœ¨è™•ç† PDFï¼Œè«‹ç¨å€™...")
        
        # è™•ç† PDF æª”æ¡ˆ
        extracted_dfs = process_pdf_file(uploaded_file)

        if extracted_dfs:
            st.success("æˆåŠŸæå–æ‰€æœ‰è¡¨æ ¼æ•¸æ“šï¼")
            st.write("ä»¥ä¸‹æ˜¯æ‰€æœ‰æå–åˆ°çš„è¡¨æ ¼æ•¸æ“š (æ¯å€‹è¡¨æ ¼ä½œç‚ºä¸€å€‹ DataFrame)ï¼š")
            
            # ä½ å¯ä»¥é¸æ“‡å¦‚ä½•åˆä½µæˆ–é¡¯ç¤ºé€™äº› DataFrame
            # ä¾‹å¦‚ï¼Œå°‡å®ƒå€‘åˆä½µæˆä¸€å€‹å¤§çš„ DataFrame (å¦‚æœçµæ§‹ç›¸å®¹)
            try:
                combined_df = pd.concat(extracted_dfs, ignore_index=True)
                st.subheader("æ‰€æœ‰è¡¨æ ¼åˆä½µå¾Œçš„æ•¸æ“š (è‹¥çµæ§‹ç›¸å®¹)")
                st.dataframe(combined_df)
                
                # æä¾›ä¸‹è¼‰é¸é …
                csv_data = combined_df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ä¸‹è¼‰æ‰€æœ‰æ•¸æ“šç‚º CSV",
                    data=csv_data,
                    file_name=f"{uploaded_file.name.replace('.pdf', '')}_extracted_data.csv",
                    mime="text/csv",
                )
            except Exception as e_concat:
                st.warning(f"ç„¡æ³•å°‡æ‰€æœ‰æå–çš„è¡¨æ ¼åˆä½µï¼š`{e_concat}`ã€‚å¯èƒ½å› ç‚ºè¡¨æ ¼çµæ§‹ä¸ä¸€è‡´ã€‚")
                st.info("æ¯å€‹å–®ç¨çš„è¡¨æ ¼å·²åœ¨ä¸Šæ–¹ç¨ç«‹é¡¯ç¤ºã€‚")
        else:
            st.warning("æœªå¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚è«‹æª¢æŸ¥ PDF å…§å®¹æˆ–å˜—è©¦èª¿æ•´ `table_settings`ã€‚")
    else:
        st.info("è«‹ä¸Šå‚³ PDF æª”æ¡ˆä»¥é–‹å§‹è™•ç†ã€‚")

if __name__ == "__main__":
    main()
